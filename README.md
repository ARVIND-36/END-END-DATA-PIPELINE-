# End-to-End Data Pipeline

A real-time data pipeline using Kafka, Debezium, PostgreSQL, MinIO, and Apache Airflow.

## ğŸš€ Quick Start

```bash
# Start all services
docker compose up -d

# Initialize Airflow database (first time only)
docker compose exec airflow-scheduler airflow db migrate

# Create Airflow admin user
docker compose exec airflow-webserver airflow users create \
    --username admin \
    --firstname FIRST_NAME \
    --lastname LAST_NAME \
    --role Admin \
    --email admin@example.org \
    --password yourpassword
```

## ğŸ“¦ Services

| Service | Port | Description |
|---------|------|-------------|
| Airflow UI | [localhost:8080](http://localhost:8080) | Workflow orchestration |
| MinIO Console | [localhost:9001](http://localhost:9001) | Object storage |
| Kafka Connect | [localhost:8083](http://localhost:8083) | CDC connector API |
| PostgreSQL | localhost:5432 | Banking database |
| Kafka | localhost:9092 / 29092 | Message broker |
| Zookeeper | localhost:2181 | Kafka coordination |

## ğŸ› ï¸ Useful Commands

```bash
# Stop all services
docker compose stop

# Start all services
docker compose start

# View logs
docker compose logs -f <service-name>

# Check service status
docker compose ps
```

## ğŸ“ Project Structure

```
â”œâ”€â”€ docker-compose.yml      # Service definitions
â”œâ”€â”€ dockerfile-airflow.dockerfile
â”œâ”€â”€ docker/
â”‚   â”œâ”€â”€ dags/               # Airflow DAGs
â”‚   â”œâ”€â”€ logs/               # Airflow logs
â”‚   â””â”€â”€ postgres/data/      # PostgreSQL data
â”œâ”€â”€ consumer/               # Kafka consumers
â”œâ”€â”€ data-generator/         # Test data generation
â””â”€â”€ banking_dbt/            # dbt models
```